#https://www.overleaf.com/project/671aa220e24f37ca532aedab
\documentclass{article}
\usepackage{geometry}
\geometry{a4paper, margin=1in}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{hyperref}

\title{Framework for Evaluating Embedding Models in Dense Retriever Search Systems}
\author{Research Team}
\date{\today}

\begin{document}

\maketitle

\section{Introduction}
The objective of this investigation is to evaluate the quality and consistency of text embedding models for dense retriever search systems by developing and comparing novel evaluation metrics. This investigation aims to capture latent properties of embedding models that predict retrieval performance, using metrics such as A-QSR-RDMD (Ambiguity by Query Semantic Region - Relevant Document Mean Deviation) and other potential measures.

The focus is on understanding the behavior of embedding models under meaning-preserving input variations, assessing their suitability for dense retrieval tasks, and ultimately improving both embedding models and retrieval systems based on these findings.

\subsection{Goals and Motivation}
\begin{itemize}
    \item \textbf{Research Problem:} How well do embedding models maintain consistency and represent semantic relationships effectively for search tasks, particularly in the context of input ambiguity?
    \item \textbf{Objective:} Develop and evaluate new metrics (e.g., A-QSR-RDMD) that assess properties such as semantic consistency and variance, with a focus on their predictive value for retrieval performance.
    \item \textbf{Impact:} Use insights gained to improve both the evaluation and training methodologies for dense retrieval systems, leading to enhanced semantic representation and handling of ambiguous queries.
\end{itemize}

\section{Experimental Setup}
\subsection{Embedding Models}
\begin{itemize}
    \item \textbf{Word2Vec} (via spaCy pipeline)
    \item \textbf{RoBERTa-based} model (via \texttt{sentence-transformers})
    \item Modular framework to incorporate additional models as needed (e.g., BERT, DPR).
\end{itemize}
These models represent a range of embedding approaches, allowing the investigation to cover both static and contextual embeddings.

\subsection{Search Dataset}
\begin{itemize}
    \item \textbf{MS MARCO V2}: A subset containing queries, qrels (relevance judgments), and corpus data.
\end{itemize}
The dataset provides a diverse set of queries and relevance labels to evaluate the robustness of different embedding models.

\section{Evaluation Metrics}
\subsection{Core Metrics}
\begin{itemize}
    \item \textbf{NDCG} (Normalized Discounted Cumulative Gain)
    \item \textbf{Recall@k}
    \item \textbf{MRR} (Mean Reciprocal Rank)
\end{itemize}
These traditional metrics provide a baseline for evaluating the quality of ranked retrieval results.

\subsection{Proposed New Measures}
\begin{itemize}
    \item \textbf{A-QSR-RDMD}: Captures the semantic consistency of embeddings within query regions by measuring the deviation of relevant document embeddings. This measure aims to reveal ambiguity in query embeddings, thereby providing insights into retrieval challenges linked to semantic variance.
    \item Additional metrics exploring semantic consistency and clustering properties will also be evaluated.
\end{itemize}

\section{Methodology}
\begin{enumerate}
    \item \textbf{Data Preparation}: Load and preprocess the MS MARCO queries, qrels, and corpus. Generate embeddings for corpus documents using pre-trained models, while generating query embeddings on-demand during retrieval.
    
    \item \textbf{Experimental Execution}:
    \begin{itemize}
        \item \textbf{Document Retrieval}: Use query embeddings to retrieve the top-k closest documents from the corpus based on a distance metric (e.g., cosine similarity).
        \item \textbf{Metric Calculation}: Calculate traditional metrics (NDCG, Recall@k, MRR) and new measures (e.g., A-QSR-RDMD) for each model.
    \end{itemize}
    
    \item \textbf{Analysis}:
    \begin{itemize}
        \item \textbf{Model Comparison}: Assess each model's performance using traditional and new metrics.
        \item \textbf{Consistency Evaluation}: Analyze the models' behavior under meaning-preserving changes (e.g., paraphrasing or synonym substitution).
        \item \textbf{Ambiguity Analysis}: Examine the relationship between ambiguity measures (e.g., A-QSR-RDMD) and retrieval metrics to identify potential improvements.
    \end{itemize}
\end{enumerate}

\section{Expected Outcomes}
\begin{itemize}
    \item \textbf{Validation of New Measures}: Demonstrate that metrics like A-QSR-RDMD capture unique properties of embedding models, such as semantic consistency and handling of ambiguity.
    \item \textbf{Model Insights}: Identify embedding models that exhibit desirable properties, such as consistent and stable centroid distances, for dense retrieval tasks.
    \item \textbf{Framework Contribution}: Propose a reproducible evaluation framework using both traditional and new metrics to inform future training schemes for embedding models.
\end{itemize}

\section{Next Steps}
\begin{enumerate}
    \item Finalize code implementations for A-QSR-RDMD and other metrics.
    \item Execute experiments with multiple models and datasets, systematically collecting results from both traditional and novel metrics.
    \item Analyze the relationship between model performance, ambiguity measures, and traditional metrics to draw actionable insights.
    \item Compile results and prepare a report summarizing key findings, insights, and potential future directions.
\end{enumerate}

\end{document}

